You are an academic paper writer. Generate a complete NEURIPS style paper
based on the experiment results provided.

════════════════════════════════════════════════════════════════════════════════
                         IMPORTANT: BEFORE YOU START
════════════════════════════════════════════════════════════════════════════════

Before writing any content, you MUST complete these steps:

1. READ THE SKILL: Review the paper-writer skill at templates/skills/paper-writer/SKILL.md
2. READ THE STYLE GUIDE: Study templates/paper_writing/lab_style_guide.md carefully
3. REVIEW EXAMPLES: Browse paper_examples/ for formatting and language patterns:
   - Look at sections/1.introduction.tex for language style
   - Look at tables/*.tex for table formatting
   - Look at commands/*.tex for macro usage
4. USE COMMAND TEMPLATES: Copy templates/paper_writing/commands/ to paper_draft/commands/

CRITICAL: Reference example papers for FORMATTING and LANGUAGE STYLE only.
Do NOT copy content, phrasing, or narrative structure from the example papers.
The examples are in a different research domain - focus only on presentation style.

════════════════════════════════════════════════════════════════════════════════
                            EXPERIMENT REPORT
════════════════════════════════════════════════════════════════════════════════

# Research Report: Semantic Interference in Multilingual Word Embeddings

## 1. Executive Summary
This research investigates the relationship between word meaning similarity and embedding similarity in multilingual large language models (MLLMs). Specifically, we address the question: &#34;If words have multiple meanings, and only one of the meanings is shared across languages, will the embedding still be similar?&#34;

Our experiments on English-French and English-Spanish pairs using the `paraphrase-multilingual-MiniLM-L12-v2` model reveal that:
1. **Meaning Alignment**: Translation pairs with shared meanings exhibit high cosine similarity (mean ~0.8), significantly above the random baseline (~0.37).
2. **Semantic Interference**: Polysemy acts as a &#34;semantic pull.&#34; There is a statistically significant negative correlation (Spearman ρ = -0.125, p = 0.010) between the number of non-shared senses and the cosine similarity of the embedding.
3. **Resilience**: Even with significant non-shared meanings, embeddings remain much closer to their translations than to random words, suggesting that the shared semantic signal remains dominant in the latent space of modern MLLMs.

## 2. Goal
The primary goal was to test whether non-shared meanings of polysemous words &#34;dilute&#34; their embedding similarity to their translations in other languages. This is crucial for understanding how MLLMs represent concepts and the potential for &#34;semantic leakage&#34; or interference in cross-lingual tasks.

## 3. Data Construction

### Dataset Description
- **MUSE Dictionaries**: Used for English-French (en-fr) and English-Spanish (en-es) translation pairs.
- **Open Multilingual WordNet (OMW)**: A sample of ~500 synsets with lemmas in multiple languages was used to provide &#34;ground truth&#34; for polysemy (sense counts) and sense sharing.

### Data Processing
1. Pairs were extracted from MUSE.
2. Only pairs where both words were present in the OMW sample were retained to ensure reliable polysemy data.
3. Identical word spellings (e.g., &#39;article&#39; in EN and FR) were excluded for the primary correlation analysis to prevent lexical overlap from confounding the semantic similarity measure.
4. Total processed non-identical pairs: 423.

### Metrics
- **Cosine Similarity**: Measured between word embeddings.
- **Non-shared Senses (NS)**: Defined as `(en_senses + target_senses) - (2 * shared_senses)`.
- **Spearman Correlation**: Used to measure the relationship between NS and similarity.

## 4. Experiment Description

### Methodology
We used `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` as a representative modern MLLM. We categorized word pairs into:
- **Monosemous-Shared**: Both words have exactly one sense, which is shared.
- **Polysemous-Full**: Both words have multiple senses, and ALL are shared.
- **Polysemous-Partial**: Words have one or more senses that are NOT shared.

### Results Summary

| Category | Mean Similarity | Std Dev | Count |
|----------|-----------------|---------|-------|
| Monosemous-Shared | 0.817 | 0.190 | 214 |
| Polysemous-Full | 0.892 | 0.109 | 10 |
| Polysemous-Partial | 0.794 | 0.177 | 49 |
| **Random Baseline** | 0.370 | 0.122 | 273 |

*Note: These stats are from the initial EN-FR run. Combined results showed similar trends.*

## 5. Result Analysis

### Key Findings
1. **Significant Negative Correlation**: In the combined dataset (n=423), the correlation between non-shared senses and similarity was **-0.125 (p=0.010)**. This confirms that extra, non-shared meanings pull the embedding away from its translation.
2. **Language Variance**: The effect was notably stronger in English-Spanish (**ρ = -0.205, p = 0.003**) than in English-French (**ρ = -0.096, p = 0.155**). This suggests that some languages may have more &#34;conflicting&#34; polysemy structures or the model is differently sensitive to them.
3. **High Baseline Similarity**: Even for words with 3+ non-shared senses, the similarity rarely dropped below 0.4-0.5, remaining far above the random baseline (~0.37). This indicates that MLLMs are robust at preserving the shared semantic signal.

### Case Studies
- **&#39;shot&#39; (EN) vs &#39;coup&#39; (FR)**: 3 non-shared senses. Similarity = **0.409**. (Low similarity due to distinct polysemy: &#39;shot&#39; as a photo/attempt vs &#39;coup&#39; as a blow/stroke).
- **&#39;substance&#39; (EN) vs &#39;substance&#39; (FR)**: Similarity = **1.0**. (Identical tokens are typically mapped to the same vector regardless of meaning divergence).

## 6. Conclusions
Our research confirms that words with similar meanings in different languages have similar embeddings, but this similarity is attenuated by polysemy. Non-shared meanings exert a measurable &#34;interference&#34; effect, reducing the alignment of the word vectors. However, the shared sense usually remains the dominant feature, keeping the vectors closer to each other than to unrelated words.

## 7. Next Steps
1. **Contextual Embeddings**: Extend this research to contextualized embeddings (ELMo/BERT) to see if context successfully &#34;prunes&#34; the non-shared meanings and restores full similarity.
2. **Larger Scale**: Use a full WordNet dataset instead of the OMW sample to increase the sample size for more languages.
3. **Causal Analysis**: Probe the embeddings to see if specific dimensions represent the non-shared meanings.


════════════════════════════════════════════════════════════════════════════════
                            RESEARCH PLAN
════════════════════════════════════════════════════════════════════════════════

# Research Plan: Multilingual Word Embedding Similarity and Polysemy

## Phase 0: Motivation &amp; Novelty Assessment

### Why This Research Matters
Understanding how multilingual models represent concepts across languages is fundamental to cross-lingual NLP. If we know how &#34;meaning&#34; is preserved or distorted in the embedding space, we can better design models for low-resource languages and improve cross-lingual transfer. Specifically, understanding how polysemy affects these representations helps in disambiguation tasks and in understanding the limitations of shared vector spaces.

### Gap in Existing Work
Most literature focuses on overall alignment (isomorphism) between languages or Bilingual Lexicon Induction (BLI) performance. While some work (like Zhang et al. 2019) touches on non-isomorphism, there is less focus on the systematic &#34;pull&#34; that non-shared meanings exert on a word&#39;s embedding in a multilingual context. We want to quantify how much a secondary, non-shared meaning &#34;dilutes&#34; the similarity of a word to its translation in another language.

### Our Novel Contribution
We will systematically compare the embedding similarity of monosemous translation pairs versus polysemous pairs where only a subset of meanings are shared. This will provide a &#34;polysemy penalty&#34; metric for cross-lingual alignment.

### Experiment Justification
- **Experiment 1: Baseline Global Alignment**: Establish the expected similarity for &#34;standard&#34; translations to provide a control group.
- **Experiment 2: Polysemy Impact Analysis**: Compare similarity scores for monosemous vs. polysemous words using WordNet (OMW) to define sense counts.
- **Experiment 3: Case Study on &#34;False Friends&#34; and &#34;Partial Friends&#34;**: Deep dive into specific word pairs (like English &#39;bank&#39; vs French &#39;banque&#39;) to see if embeddings cluster more with the shared or non-shared meanings.

---

## Phase 1: Planning

### Research Question
Do words with similar meanings in different languages have similar embeddings in multilingual models? Specifically, how does polysemy (multiple meanings) affect this similarity when only some meanings are shared?

### Hypothesis Decomposition
1. **H1 (Similarity)**: Translation pairs for monosemous words will exhibit significantly higher cosine similarity than random word pairs in MLLMs.
2. **H2 (Polysemy Interference)**: Translation pairs where one or both words are polysemous will have lower similarity than monosemous pairs, proportional to the number of non-shared senses.
3. **H3 (Dominant Sense)**: The embedding of a polysemous word will be closer to its translation if the shared sense is the &#34;dominant&#34; (most frequent) sense in both languages.

### Proposed Methodology

#### Approach
We will use a modern multilingual model (e.g., `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` or `xlm-roberta-base`) to extract word embeddings. We will use WordNet (via the OMW sample provided) to categorize words by their polysemy (number of synsets they belong to).

#### Experimental Steps
1. **Setup**: Initialize environment and load models.
2. **Data Processing**: 
    - Extract English-French and English-Spanish pairs from MUSE.
    - Annotate these pairs with &#34;Sense Count&#34; from WordNet/OMW.
    - Identify &#34;Shared Senses&#34; vs &#34;Unique Senses&#34;.
3. **Similarity Calculation**:
    - Calculate cosine similarity for all pairs.
    - Group results by polysemy levels (Monosemous-Monosemous, Monosemous-Polysemous, Polysemous-Polysemous).
4. **Statistical Analysis**: 
    - Perform T-tests/ANOVA to see if polysemy significantly reduces similarity.
    - Correlation analysis between (Sense Count) and (Similarity).

### Baselines
- **Random Pairs**: Similarity between non-translation pairs.
- **Monosemous Translations**: The &#34;Gold Standard&#34; for similarity.

### Evaluation Metrics
- **Cosine Similarity**: Primary metric for embedding closeness.
- **CSLS (Cross-Domain Similarity Local Scaling)**: To account for the &#34;hubness&#34; problem in high-dimensional spaces.

### Statistical Analysis Plan
- Correlation (Spearman) between total senses and cosine similarity.
- T-test comparing similarity of Monosemous vs Polysemous pairs.

## Success Criteria
- Successfully quantified the relationship between polysemy and cross-lingual embedding similarity.
- Identified whether &#34;partial&#34; meaning similarity is sufficient for &#34;high&#34; embedding similarity.


════════════════════════════════════════════════════════════════════════════════
                          LITERATURE REVIEW
════════════════════════════════════════════════════════════════════════════════

# Literature Review: Similarity of Word Embeddings Across Languages

## Research Area Overview
The study of cross-lingual word embeddings (CLWE) and multilingual large language models (MLLMs) aims to create a shared vector space where words with similar meanings from different languages are represented by similar vectors. This field has evolved from alignment-based methods for static embeddings (e.g., MUSE) to implicitly aligned representations in pretrained transformers (e.g., mBERT, XLM-R).

## Key Papers

### 1. Emerging Cross-lingual Structure in Pretrained Language Models (2019)
- **Authors**: Wu et al.
- **Key Contribution**: Shows that cross-lingual transfer is possible in MLLMs even without shared vocabulary, suggesting emergent universal latent symmetries.
- **Methodology**: Analyzes MLLMs and shows that monolingual BERT models can be aligned post-hoc effectively.
- **Relevance**: Supports the hypothesis that words with similar meanings have similar embeddings.

### 2. A Survey of Cross-lingual Word Embedding Models (2017)
- **Authors**: Ruder et al.
- **Key Contribution**: Provides a comprehensive typology of CLWE models, comparing their data requirements and objective functions.
- **Relevance**: Foundational for understanding different alignment strategies.

### 3. Concept Space Alignment in Multilingual LLMs (2024)
- **Authors**: Peng &amp; Søgaard
- **Key Contribution**: Evaluates concept alignment in modern LLMs (Llama-2, BLOOMZ, etc.) and finds high-quality linear alignments.
- **Methodology**: Uses WordNet synsets as concepts and Procrustes analysis for alignment.
- **Results**: Generalization works best for typologically similar languages and for abstract concepts.

### 4. Brains and language models converge on a shared conceptual space across different languages (2025)
- **Authors**: Zada et al.
- **Key Contribution**: Finds that both brains and LMs converge on a shared conceptual space regardless of the language.
- **Relevance**: Provides strong cross-disciplinary evidence for the hypothesis.

### 5. Are Girls Neko or Shōjo? (2019)
- **Authors**: Zhang et al.
- **Key Contribution**: Addresses the issue of non-isomorphism in embedding spaces using Iterative Normalization.
- **Relevance**: Relevant to the part of the hypothesis about multiple meanings and how they might affect alignment.

## Common Methodologies
- **Procrustes Analysis**: Finding a linear transformation (rotation) to align two embedding spaces using a seed dictionary.
- **Unsupervised Alignment (MUSE)**: Using adversarial training and Procrustes to align spaces without parallel data.
- **WordNet Alignment**: Using multilingual WordNets (OMW) to define parallel concepts.

## Evaluation Metrics
- **Bilingual Lexicon Induction (BLI)**: Accuracy of translating words using nearest neighbor search in the shared space.
- **Word Similarity**: Correlation with human judgments (e.g., SimLex-999).
- **Cross-lingual Word Similarity**: Measuring similarity between word pairs in different languages.

## Recommended for Our Experiment
- **Datasets**: MUSE dictionaries (en-fr, en-es, en-de), OMW (WordNet) for concept-based evaluation, and False Friends dataset for testing multiple meanings.
- **Baselines**: MUSE unsupervised alignment, Procrustes on mBERT/Llama-2 embeddings.
- **Hypothesis Testing**: Focus on words with multiple meanings (polysemy) using WordNet to identify shared vs. distinct senses.


════════════════════════════════════════════════════════════════════════════════
                          PAPER REQUIREMENTS
════════════════════════════════════════════════════════════════════════════════

Generate a complete academic paper with the following structure:

1. TITLE
   - Clear, specific, informative
   - Should convey main finding or contribution

2. ABSTRACT (150-250 words)
   - Problem statement
   - Approach
   - Key results
   - Significance

3. INTRODUCTION
   - Research problem and motivation
   - Gap in existing work
   - Our contribution (be specific)
   - Paper organization

4. RELATED WORK
   - Organized by theme/approach
   - Position our work relative to prior work
   - Cite papers from literature review

5. METHODOLOGY
   - Clear description of approach
   - Experimental setup
   - Datasets used
   - Evaluation metrics
   - Baselines

6. RESULTS
   - Present results with tables and figures
   - Statistical analysis
   - Comparison to baselines
   - Ablation studies (if applicable)

7. DISCUSSION
   - Interpretation of results
   - Limitations
   - Broader implications

8. CONCLUSION
   - Summary of contributions
   - Key findings
   - Future work

9. REFERENCES
   - BibTeX format
   - All cited papers

════════════════════════════════════════════════════════════════════════════════
                          OUTPUT FORMAT
════════════════════════════════════════════════════════════════════════════════

Create a MODULAR LaTeX project with the following directory structure:

paper_draft/
├── main.tex              # Main file that imports all sections
├── references.bib        # BibTeX references
├── sections/
│   ├── abstract.tex      # Abstract content
│   ├── introduction.tex  # Introduction section
│   ├── related_work.tex  # Related work section
│   ├── methodology.tex   # Methodology section
│   ├── results.tex       # Results section
│   ├── discussion.tex    # Discussion section
│   └── conclusion.tex    # Conclusion section
├── figures/              # Directory for any generated figures
├── tables/               # Directory for complex standalone tables
└── appendix/             # Directory for appendix sections (if needed)

INSTRUCTIONS:
1. First, create the directory structure above (mkdir -p paper_draft/sections paper_draft/figures paper_draft/tables paper_draft/appendix)
2. Write main.tex using the EXACT preamble for NEURIPS:

   \documentclass{article}
   \usepackage[final]{neurips_2025}  % NEURIPS style (neurips_2025.sty is in paper_draft/)
   \usepackage[hidelinks]{hyperref}  % REQUIRED: clickable links
   \usepackage{booktabs}  % REQUIRED: professional tables
   \usepackage{graphicx}
   \usepackage{amsmath,amssymb}

   % Import command files
   \input{commands/math}
   \input{commands/general}
   \input{commands/macros}

   % Use this bibliography style:
   \bibliographystyle{plainnat}

   - Use \input{sections/...} to include each section
   - Use \bibliography{references} for references
3. Write each section file with COMPLETE content (no placeholders)
4. Each section file should include its \section{} command
5. Write references.bib with all citations in BibTeX format
6. After writing all files, compile the paper:
   cd paper_draft && pdflatex -interaction=nonstopmode main.tex && bibtex main && pdflatex -interaction=nonstopmode main.tex && pdflatex -interaction=nonstopmode main.tex

This modular structure allows humans to easily:
- Edit individual sections without navigating a large file
- Track changes per section
- Reuse sections across different paper versions

════════════════════════════════════════════════════════════════════════════════
                          QUALITY REQUIREMENTS
════════════════════════════════════════════════════════════════════════════════

- Academic tone throughout
- All claims must be supported by data from the experiment report
- Proper citations using \cite{} commands
- Clear figures and tables with proper captions
- NO placeholder text - every section must have real content
- The paper MUST compile without errors
- If compilation fails, debug and fix the LaTeX errors

════════════════════════════════════════════════════════════════════════════════
                          LAB WRITING STYLE
════════════════════════════════════════════════════════════════════════════════

Follow these lab-specific conventions to match our paper style:

1. LANGUAGE STYLE:
   - Use active voice: "We propose", "We examine", "We focus on"
   - Be direct and confident: "Our main question is...", "We hypothesize that..."
   - State things clearly and simply - prefer plain language over jargon
   - Use bold questions as paragraph organizers: {\bf what is X?}
   - Include specific quantitative claims: "8.97% over baseline"
   - Avoid fancy wording: "utilize" → "use", "facilitate" → "help"

2. INTRODUCTION STRUCTURE:
   - Engaging hook (get to the point quickly)
   - Problem importance
   - Gap identification
   - Your approach with method figure reference
   - Quantitative preview of results
   - Contribution bullets (3-4 items, action verbs)

3. CONTRIBUTION LISTS:
   \begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
       \item We propose...
       \item We conduct...
       \item We complement...
   \end{itemize}

4. MODULAR COMMANDS STRUCTURE:
   Create paper_draft/commands/ directory with:
   - math.tex: Math notation macros (copy from templates/paper_writing/commands/)
   - general.tex: Formatting macros (copy from templates/paper_writing/commands/)
   - macros.tex: Project-specific term definitions

   In main.tex, include:
   \input{commands/math}
   \input{commands/general}
   \input{commands/macros}

5. REFERENCE CONVENTIONS:
   Use reference macros from math.tex:
   - \figref{fig:name} for "figure 1" (lowercase, in-sentence)
   - \Figref{fig:name} for "Figure 1" (capitalized, start of sentence)
   - \secref{sec:name} for "section 2"

6. TEXT FORMATTING:
   - Use \para{Header text} for bold paragraph headers
   - Define method/dataset names with \textsc and \xspace:
     \newcommand{\methodname}{\textsc{MethodName}\xspace}

7. TABLE FORMATTING:
   - Use booktabs package (no vertical lines)
   - Use \resizebox{\textwidth}{!}{...} for wide tables
   - Use @{} to remove padding at table edges
   - Use \cmidrule(lr){x-y} for sub-headers
   - Use \textsc{} for dataset/method names in headers
   - Bold best results with {\bf ...}

8. FIGURE FORMATTING:
   - Use 0.32\textwidth for 3-column subfigures
   - Use 0.95\linewidth for full-width figures
   - Use \input{figures/legend} for shared legends
   - Write self-contained captions explaining key observations

9. RESULTS PRESENTATION:
   - Define \increase and \decrease for colored arrows (green up, red down)
   - Bold best results in tables
   - Report confidence intervals when available

10. ALGORITHM STYLING:
    - Use algpseudocode with [noend]
    - Use \triangleright for comments

11. HYPERLINKS (REQUIRED):
    - Always use \usepackage[hidelinks]{hyperref} or with colored links
    - All citations, section refs, figure refs, table refs must be clickable
    - This is essential for reader navigation

════════════════════════════════════════════════════════════════════════════════
                          WORKFLOW: REVIEW AND REFLECT
════════════════════════════════════════════════════════════════════════════════

Before calling finish, you MUST complete these review steps:

1. REVIEW RESOURCES (at the start):
   - Read templates/skills/paper-writer/SKILL.md for detailed guidance
   - Study templates/paper_writing/lab_style_guide.md for style conventions
   - Browse paper_examples/ for formatting and language patterns

2. SELF-REFLECTION (before finishing):
   After writing all sections, review your work against these criteria:

   LANGUAGE CHECK:
   - [ ] Is the writing clear and jargon-free?
   - [ ] Are claims specific with quantitative support?
   - [ ] Is active voice used throughout?

   FORMATTING CHECK:
   - [ ] Does main.tex include \input{commands/math}, \input{commands/general}, \input{commands/macros}?
   - [ ] Is hyperref package included for clickable references?
   - [ ] Do tables use booktabs (no vertical lines)?
   - [ ] Are best results bolded in tables?
   - [ ] Are figures/tables properly captioned?

   STRUCTURE CHECK:
   - [ ] Does introduction follow: hook → importance → gap → approach → preview → contributions?
   - [ ] Are contribution bullets specific with action verbs?
   - [ ] Does the paper compile without errors?

3. FIX ISSUES:
   - Address any issues found in the self-reflection
   - Re-compile and verify the PDF looks correct

Only after completing this review should you consider the paper finished.