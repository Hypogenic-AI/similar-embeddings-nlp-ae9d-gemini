# Downloaded Papers

1. [Emerging Cross-lingual Structure in Pretrained Language Models](Emerging_Cross_lingual_Structure.pdf)
   - Authors: Shijie Wu, Alexis Conneau, Haoran Li, Luke Zettlemoyer, Veselin Stoyanov
   - Year: 2019
   - arXiv: 1911.01464
   - Why relevant: Discusses how cross-lingual structure emerges even without shared vocabulary.

2. [A Survey of Cross-lingual Word Embedding Models](Survey_Cross_lingual_Word_Embeddings.pdf)
   - Authors: Sebastian Ruder, Ivan Vulić, Anders Søgaard
   - Year: 2017
   - arXiv: 1706.04902
   - Why relevant: Comprehensive survey of methods for aligning embedding spaces.

3. [Unsupervised Multilingual Word Embeddings](Unsupervised_Multilingual_Word_Embeddings.pdf)
   - Authors: Xilun Chen, Claire Cardie
   - Year: 2018
   - arXiv: 1805.03613
   - Why relevant: Proposes a framework for learning MWEs by exploiting relations between all language pairs.

4. [Are Girls Neko or Shōjo? Cross-Lingual Alignment of Non-Isomorphic Embeddings with Iterative Normalization](Are_Girls_Neko_or_Shojo.pdf)
   - Authors: Mozhi Zhang, Keyulu Xu, K. Kawarabayashi, S. Jegelka, Jordan L. Boyd-Graber
   - Year: 2019
   - arXiv: 1901.02444
   - Why relevant: Addresses non-isomorphism in embedding spaces, which is crucial for my hypothesis about multiple meanings.

5. [Multi-SimLex: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity](Multi_SimLex.pdf)
   - Authors: Ivan Vulić et al.
   - Year: 2020
   - arXiv: 2003.04866
   - Why relevant: Provides a large-scale benchmark for cross-lingual word similarity.

6. [Word Translation Without Parallel Data](Word_Translation_Without_Parallel_Data.pdf)
   - Authors: Alexis Conneau et al.
   - Year: 2017
   - arXiv: 1710.04087
   - Why relevant: Describes the MUSE method for unsupervised alignment.

7. [Concept Space Alignment in Multilingual LLMs](Concept_Space_Alignment.pdf)
   - Authors: Qiwei Peng, Anders Søgaard
   - Year: 2024
   - arXiv: 2409.04313
   - Why relevant: Modern evaluation of concept alignment in LLMs, specifically mentioning abstract concepts.

8. [Brains and language models converge on a shared conceptual space across different languages](Brains_and_LMs_Convergence.pdf)
   - Authors: Zaid Zada, Samuel A. Nastase, Jixing Li, Uri Hasson
   - Year: 2025
   - arXiv: 2506.20489
   - Why relevant: Shows convergence on a shared conceptual space across different languages in both brains and LMs.
